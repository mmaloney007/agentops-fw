config_version: "0.2.0"
config_preset: "4090-llama3.1-8b"
base_model: "meta-llama/Meta-Llama-3.1-8B-Instruct"
tasks: "tasks/robust_eval_gold.jsonl"
out: ""
steps: 2000
max_prompt_len: 1024
max_new_tokens: 192
temperature: 0.7
top_p: 0.95
lr: 1.0e-5
weight_decay: 0.01
gradient_accumulation: 2
max_grad_norm: 1.0
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
lora_targets: ""
load_in_4bit: true
torch_dtype: "bfloat16"
eval_interval: 50
deterministic: false
force_json_fallback: false
lam_latency: 0.0
mu_cost: 0.0
gamma_stability: 0.0
seed: 1234
repro: true
cache_dataset: true
cache_dir: "out/cache"
checkpoint_every: 200
resume_from: null
no_silent_defaults: true
expected_dataset_hash: null
allow_dataset_drift: false
