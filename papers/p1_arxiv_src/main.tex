\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\title{Stable, SLO-Aware Tool-Using Agents on a Single GPU}
\author{Mike Maloney \\ Neuralift \\ University of New Hampshire}
\date{2025-11-20}
\begin{document}
\maketitle

\begin{abstract}
We present a single-GPU recipe for stable, SLO-aware tool-using agents that runs on a MacBook Pro (LM Studio / Metal) and an Ubuntu RTX 4090 box (Ollama / HF local). Our framework combines programmatic rewards for schema validity and task success with penalties for tail latency and token usage. A stability harness quantifies answer drift via hash-based disagreement rates across repeated runs. We report latency histograms, QPS-to-p95 curves, success@SLO curves, and stability metrics across providers and thought budgets, yielding a practical blueprint for reproducible, auditable agent behavior with open-weight models on commodity hardware.
\end{abstract}

\section{Introduction}
Large language model agents deployed in enterprise contexts must satisfy three constraints: output shape (schema/tool contracts), service-level objectives (SLOs) on latency and cost, and stability (answers do not drift unpredictably across runs and time). We operationalize these constraints on commodity hardware via deterministic decoding, JSON-schema enforcement, explicit SLO metrics (p95/p99, success@SLO), and an answer stability harness.

\section{Related Work}
Structured outputs and programmatic rewards enable verifiable evaluation without human raters. SLO engineering emphasizes tail latency over averages \citep{tail_at_scale}. Efficient finetuning (\citealp{qlora}) and RL libraries (\citealp{trl}) make single-GPU alignment feasible. Local runtime stacks include LM Studio and Ollama \citep{lmstudio,ollama}. We log all experiments to Weights \& Biases \citep{wandb}.

\section{Method}
We expose providers for LM Studio (Mac), Ollama (Ubuntu), and HF local. For a task with JSON Schema $S$ and output $y$, we compute
\begin{equation}
R = R_{\text{schema}} + R_{\text{succ}} - \lambda L - \mu C - \gamma D,
\end{equation}
where $L$ is latency (seconds), $C$ token cost (thousands), and $D$ the disagreement rate from repeated runs.

\section{Experiments}
We report latency histograms (Fig.~\ref{fig:hist}), QPS-to-p95 curves (Fig.~\ref{fig:qps}), and success@SLO curves (Fig.~\ref{fig:slo}). Stability is summarized as median and worst-case disagreement across prompts.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/latency_hist.png}
  \caption{Latency histogram for baseline runs (placeholder until data are generated).}
  \label{fig:hist}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/qps_vs_p95.png}
  \caption{Effective QPS vs p95 latency (placeholder).}
  \label{fig:qps}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/success_at_slo.png}
  \caption{success@SLO across thresholds (placeholder).}
  \label{fig:slo}
\end{figure}

\section{Discussion}
The recipe yields stable, schema-valid outputs with controllable SLOs on a single GPU. We observe predictable trade-offs between thought budgets and tail latency, and between stability and throughput.

\bibliographystyle{plainnat}
\bibliography{refs}
\end{document}
