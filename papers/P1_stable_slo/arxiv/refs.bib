@article{dean2013tail,
  title={The Tail at Scale},
  author={Dean, Jeffrey and Barroso, Luiz},
  journal={Communications of the ACM},
  year={2013},
  url={https://cacm.acm.org/research/the-tail-at-scale/}
}

@article{kwon2023vllm,
  title={Efficient Memory Management for Large Language Model Serving with {vLLM}},
  author={Kwon, Youngmin and others},
  year={2023},
  eprint={2309.06180},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2309.06180}
}

@article{zheng2023sglang,
  title={{SGLang}: Efficient Serving of {LLM} Applications},
  author={Zheng, Lianmin and others},
  year={2023},
  eprint={2312.07104},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2312.07104}
}

@article{leviathan2023speculative,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and others},
  year={2023},
  eprint={2211.17192},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2211.17192}
}

@article{dao2023flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri and others},
  year={2023},
  eprint={2307.08691},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2307.08691}
}

@article{deepspeedFastGen,
  title={{DeepSpeed-FastGen}: High Throughput Text Generation for {LLMs}},
  author={Microsoft DeepSpeed Team},
  year={2024},
  eprint={2401.08671},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2401.08671}
}

@article{prabhu2024vattention,
  title={v{A}ttention: Dynamic Sparse Attention with Low-rank Projections},
  author={Prabhu, Ameya and others},
  year={2024},
  eprint={2405.04437},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2405.04437}
}

@article{geng2025jsonschemabench,
  title={JSONSchemaBench: Evaluating Structured Output for {LLMs}},
  author={Geng, Xi and others},
  year={2025},
  eprint={2501.10868},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2501.10868}
}

@article{letmespeakfreely,
  title={Let Me Speak Freely: Lessons from Free-form Generation with Structure},
  author={Anonymous},
  year={2024},
  eprint={2408.02442},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2408.02442}
}

@article{structuredRAG,
  title={Structured {RAG}: Structured Outputs for Retrieval-Augmented Generation},
  author={Anonymous},
  year={2024},
  eprint={2408.11061},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2408.11061}
}

@misc{outlinesDocs,
  title={Outlines: Structured Generation with {LLMs}},
  howpublished={\url{https://dottxt-ai.github.io/outlines/}},
  year={2024}
}

@misc{llamaCppGrammar,
  title={{llama.cpp} Grammars},
  howpublished={\url{https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md}},
  year={2024}
}

@misc{lmstudioTools,
  title={{LM} Studio OpenAI-Compatible Tools},
  howpublished={\url{https://lmstudio.ai/docs/developer/openai-compat/tools}},
  year={2024}
}

@misc{openaiStructured,
  title={Introducing Structured Outputs in the API},
  howpublished={\url{https://openai.com/index/introducing-structured-outputs-in-the-api/}},
  year={2023}
}

@misc{ollamaStructured,
  title={Structured Outputs in Ollama},
  howpublished={\url{https://blog.danielclayton.co.uk/posts/ollama-structured-outputs/}},
  year={2024}
}

@misc{fireworksStructured,
  title={Why Do All {LLMs} Need Structured Output Modes?},
  howpublished={\url{https://fireworks.ai/blog/why-do-all-LLMs-need-structured-output-modes}},
  year={2024}
}

@article{liu2022structured,
  title={Structured Decoding for Large Language Models},
  author={Liu, X. and others},
  year={2022},
  eprint={2203.11171},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2203.11171}
}

@misc{instructorLlamaCpp,
  title={Instructor + {llama.cpp}},
  howpublished={\url{https://python.useinstructor.com/integrations/llama-cpp-python/}},
  year={2024}
}

@misc{modalVllm,
  title={Deploying v{LLM} on Modal},
  howpublished={\url{https://modal.com/docs/examples/vllm_inference}},
  year={2024}
}

@misc{wandbQuickstart,
  title={Weights \& Biases Models Quickstart},
  howpublished={\url{https://docs.wandb.ai/models/quickstart}},
  year={2024}
}

@misc{wandbArtifacts,
  title={Weights \& Biases Artifacts},
  howpublished={\url{https://docs.wandb.ai/models/artifacts}},
  year={2024}
}

@misc{wandbEvaluate,
  title={Evaluate Models with Weights \& Biases},
  howpublished={\url{https://docs.wandb.ai/models/evaluate-models}},
  year={2024}
}

@misc{agrawal2024sarathi,
  title={Sarathi-Serve: High Throughput Inference for Large Language Models},
  author={Agrawal, A. and others},
  year={2024},
  howpublished={\url{https://www.usenix.org/system/files/osdi24-agrawal.pdf}},
  note={OSDI'24}
}

@article{oss204667,
  title={Open-Source Serving Techniques for {LLMs}},
  author={Anonymous},
  year={2024},
  eprint={2408.04667},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2408.04667}
}

@misc{qwenDocs,
  title={{Qwen} Documentation},
  howpublished={\url{https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html}},
  year={2024}
}

@misc{lmstudioCatalog,
  title={{LM} Studio Model Catalog},
  howpublished={\url{https://lmstudio.ai/models}},
  year={2024}
}

@article{es2023ragas,
  title={{RAGAS}: Automated Evaluation of Retrieval-Augmented Generation},
  author={Es, Tanay and others},
  year={2023},
  eprint={2309.15217},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2309.15217}
}

@article{atomicFacts2024,
  title={Evaluating Atomic Facts in LLM Outputs},
  author={Anonymous},
  year={2024},
  eprint={2408.15171},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2408.15171}
}

@article{schulman2017ppo,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and others},
  year={2017},
  eprint={1707.06347},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1707.06347}
}

@article{grpo,
  title={{GRPO}: Generalized Reinforcement Policy Optimization},
  author={Anonymous},
  year={2022},
  eprint={2203.02155},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2203.02155}
}

@article{llmjudgebias,
  title={Are Large Language Models Fair Evaluators?},
  author={Anonymous},
  year={2023},
  eprint={2305.17926},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2305.17926}
}

@article{llmStability,
  title={On Stability and Variance in LLM Evaluations},
  author={Anonymous},
  year={2024},
  eprint={2408.04667},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2408.04667}
}

@inproceedings{achiam2017cpo,
  title={Constrained Policy Optimization},
  author={Achiam, Joshua and others},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  year={2017},
  url={https://proceedings.mlr.press/v70/achiam17a.html}
}

@book{altman1999cmdp,
  title={Constrained Markov Decision Processes},
  author={Altman, Eitan},
  publisher={Chapman and Hall/CRC},
  year={1999}
}

@article{dettmers2023qlora,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  author={Dettmers, Tim and others},
  year={2023},
  eprint={2305.14314},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2305.14314}
}
